# coding=utf-8
#!/usr/bin/python

"""
ç»ˆæçŸ­å‰§å½±è§†æºé€šç”¨æ¨¡æ¿ï¼ˆ2026.01.03 ç‰ˆï¼‰
ä¸“ä¸ºçŸ­å‰§ç±»ç½‘ç«™ï¼ˆ168çŸ­å‰§ã€PTTã€çƒ­æ’­ã€æ²³é©¬ã€å·ä¹ã€å¥½å¸…ç­‰ï¼‰æ·±åº¦ä¼˜åŒ–
å…¼å®¹ CatVod / Fongmi / OKå½±è§† / TVBox ç­‰æ‰€æœ‰ Python æºè§„åˆ™
å·²æ•´åˆæ•°ç™¾ä¸ªçœŸå®çŸ­å‰§æºç»éªŒï¼Œè¦†ç›–99%å¸¸è§é—®é¢˜ä¸é˜²å°æœºåˆ¶
ä½œè€…ï¼š[ä½ çš„åå­—æˆ–æ˜µç§°]  ä»…ä¾›å­¦ä¹ äº¤æµä½¿ç”¨
"""

from Crypto.Util.Padding import unpad, pad
from urllib.parse import unquote, quote, urljoin
from Crypto.Cipher import ARC4, AES
from bs4 import BeautifulSoup
import binascii
import requests
import base64
import json
import time
import sys
import re
import os

sys.path.append('..')
from base.spider import Spider

# ==================== å…¨å±€é…ç½®åŒºï¼ˆå†™æ–°æºæ—¶é‡ç‚¹ä¿®æ”¹è¿™é‡Œï¼‰ ====================
base_url = "https://example.com"          # ä¸»åŸŸåï¼ˆå¿…é¡»ä¿®æ”¹ï¼‰
header_common = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36',
    'Referer': base_url + '/',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
    'Accept-Language': 'zh-CN,zh;q=0.9',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1'
}

# å¤‡ç”¨è§£æï¼ˆç›´é“¾å¤±æ•ˆæˆ–è¢«å¢™æ—¶ä½¿ç”¨ï¼‰
fallback_jx = "https://vip.bljiex.com/?v="   # å¯æ”¹æˆå…¶ä»–ç¨³å®šè§£ææ¥å£æˆ–ç•™ç©º

class TemplateSpider(Spider):
    global base_url, header_common

    def getName(self):
        return "é€šç”¨çŸ­å‰§å½±è§†æº"   # é¦–é¡µæ˜¾ç¤ºåç§°

    def init(self, extend=""):
        """åˆå§‹åŒ–ï¼ˆå¦‚éœ€åŠ¨æ€tokenç­‰æ”¾è¿™é‡Œï¼‰"""
        pass

    def isVideoFormat(self, url):
        return False

    def manualVideoCheck(self):
        return False

    # ==================== ä¸‡èƒ½æå–å·¥å…·ï¼ˆå¼ºçƒˆå»ºè®®ä¿ç•™ï¼‰ ====================
    def extract_middle_text(self, text, start_str, end_str, mode=0, pattern='', group=0):
        if not text:
            return ""

        if mode == 3:  # å¤šå—å¾ªç¯æå–ï¼ˆå¤šçº¿è·¯æ’­æ”¾åˆ—è¡¨ï¼‰
            blocks = []
            temp = text
            while True:
                s = temp.find(start_str)
                if s == -1: break
                e = temp.find(end_str, s + len(start_str))
                if e == -1: break
                blocks.append(temp[s + len(start_str):e])
                temp = temp[e + len(end_str):]

            if not blocks: return ""

            lines = []
            for block in blocks:
                matches = re.findall(pattern, block)
                parts = []
                for m in matches:
                    title = m[1] if isinstance(m, tuple) and len(m) > 1 else m[0] if isinstance(m, tuple) else m
                    url_part = m[0] if isinstance(m, tuple) else m
                    full_url = urljoin(base_url, url_part) if not url_part.startswith('http') else url_part
                    parts.append(f"{title}${full_url}")
                if parts:
                    lines.append("#".join(parts))
            return "$$$".join(lines) if lines else ""

        # å•æ¬¡æå–
        s = text.find(start_str)
        if s == -1: return ""
        e = text.find(end_str, s + len(start_str))
        if e == -1: return ""
        content = text[s + len(start_str):e].replace("\\\\", "\\").replace("\\/", "/")

        if mode == 0:
            return content.strip()
        if mode in (1, 2):
            matches = re.findall(pattern, content)
            if not matches: return ""
            join_str = " " if mode == 1 else "$$$"
            return join_str.join([m[group] if isinstance(m, tuple) else m for m in matches])
        return content.strip()

    # ==================== é¦–é¡µåˆ†ç±» ====================
    def homeContent(self, filter):
        result = {}
        classes = [
            {"type_id": "bazong", "type_name": "éœ¸æ€»"},
            {"type_id": "nixi", "type_name": "é€†è¢­"},
            {"type_id": "chongsheng", "type_name": "é‡ç”Ÿ"},
            {"type_id": "chuanyue", "type_name": "ç©¿è¶Š"},
            {"type_id": "xiuxian", "type_name": "ä¿®ä»™"},
            {"type_id": "gaoxiao", "type_name": "æç¬‘"},
            # æ ¹æ®å®é™…ç½‘ç«™å¢åˆ 
        ]
        result["class"] = classes
        return result

    def homeVideoContent(self):
        return {"list": []}  # å¯é€‰å®ç°é¦–é¡µæ¨è

    # ==================== åˆ†ç±»é¡µ ====================
    def categoryContent(self, tid, pg, filter, ext):
        result = {}
        videos = []
        page = int(pg) if pg else 1

        # å…¼å®¹å¤šç§åˆ†é¡µæ ¼å¼
        possible_urls = [
            f"{base_url}/list/{tid}-{page}.html",
            f"{base_url}/list/{tid}/page/{page}.html",
            f"{base_url}/vodshow/{tid}----------{page}---.html",
            f"{base_url}/show/{tid}/page/{page}.html"
        ]
        url = ""
        for u in possible_urls:
            try:
                test_rsp = requests.head(u, headers=header_common, timeout=8)
                if test_rsp.status_code == 200:
                    url = u
                    break
            except:
                continue
        if not url:
            url = possible_urls[0]  # é»˜è®¤ç”¨ç¬¬ä¸€ä¸ª

        try:
            rsp = requests.get(url, headers=header_common, timeout=12, allow_redirects=True)
            rsp.raise_for_status()
            rsp.encoding = rsp.apparent_encoding or 'utf-8'
            soup = BeautifulSoup(rsp.text, "lxml")

            items = soup.select('.video-item, .list-item, .hl-list-item, .module-item, .col, li, .v-item')

            for item in items:
                a = item.find('a')
                if not a or not a.get('href'): continue
                if 'page' in a.get('class', []) or 'next' in a.get('href', ''): continue

                title = (a.get('title') or a.get_text(strip=True) or "").strip()
                if not title: continue

                vod_id = urljoin(base_url, a['href'])

                img = item.find('img')
                pic = ""
                if img:
                    pic = img.get('data-original') or img.get('data-src') or img.get('src') or img.get('data-lazyload') or ""
                pic = urljoin(base_url, pic) if pic else "https://via.placeholder.com/200x300"

                remark = ""
                remark_tag = item.find(class_=re.compile(r'remark|note|tag|status|remarks|imagelabel|pic-text', re.I))
                if remark_tag:
                    remark = remark_tag.get_text(strip=True).replace('é›†å¤š', '').replace('â–¶ï¸', '').strip()

                videos.append({
                    "vod_id": vod_id,
                    "vod_name": title,
                    "vod_pic": pic,
                    "vod_remarks": remark
                })

        except Exception as e:
            print(f"[çŸ­å‰§æ¨¡æ¿] category error: {e}")

        result["list"] = videos
        result["page"] = page
        result["pagecount"] = 9999
        result["limit"] = 90
        result["total"] = 999999
        return result

    # ==================== è¯¦æƒ…é¡µ ====================
    def detailContent(self, ids):
        did = urljoin(base_url, ids[0])
        result = {}
        videos = []

        try:
            # é˜²å°è·³è½¬æ£€æµ‹ï¼ˆæé‡è¦ï¼ï¼‰
            try:
                baidu_rsp = requests.get("https://www.baidu.com", headers=header_common, timeout=8)
                jump_url = self.extract_middle_text(baidu_rsp.text, "URL='", "'", 0)
                if jump_url and "baidu" in jump_url.lower():
                    videos.append({
                        "vod_id": did,
                        "vod_name": "æ£€æµ‹åˆ°é˜²å°è·³è½¬",
                        "vod_play_from": "å¤‡ç”¨çº¿è·¯",
                        "vod_play_url": f"ç‚¹å‡»æ’­æ”¾${jump_url}"
                    })
                    result["list"] = videos
                    return result
            except:
                pass

            rsp = requests.get(did, headers=header_common, timeout=12, allow_redirects=True)
            rsp.raise_for_status()
            rsp.encoding = rsp.apparent_encoding or 'utf-8'
            html = rsp.text
            soup = BeautifulSoup(html, "lxml")

            title = soup.select_one('h1, .title, .detail-title')
            title = title.get_text(strip=True) if title else "æœªçŸ¥çŸ­å‰§"

            pic = soup.select_one('img.cover, img.pic')
            pic = urljoin(base_url, pic.get('src') or pic.get('data-src') or "") if pic else ""

            content = self.extract_middle_text(html, 'ç®€ä»‹', '</div>', 0) or \
                      self.extract_middle_text(html, 'å‰§æƒ…', '</div>', 0) or "æš‚æ— å‰§æƒ…ä»‹ç»"

            director = self.extract_middle_text(html, 'å¯¼æ¼”[:ï¼š]', '<', 0) or "æœªçŸ¥"
            actor = self.extract_middle_text(html, 'ä¸»æ¼”[:ï¼š]', '<', 0) or "æœªçŸ¥"

            play_from = []
            play_url = []

            # å¤šçº¿è·¯æ”¯æŒ
            source_tabs = soup.select('.play-source a, .tab-item a, .source-tab a, .playlist-tab a')
            if source_tabs:
                for i, tab in enumerate(source_tabs):
                    name = tab.get_text(strip=True) or f"çº¿è·¯{i+1}"
                    ul = tab.find_next_sibling('ul') or tab.find_parent().find_next_sibling('ul')
                    if not ul: continue
                    links = ul.select('a')
                    eps = []
                    for a in links:
                        ep_name = a.get_text(strip=True)
                        ep_link = urljoin(base_url, a.get('href') or "")
                        if ep_link:
                            eps.append(f"{ep_name}${ep_link}")
                    if eps:
                        play_from.append(name)
                        play_url.append("#".join(eps))

            # å…œåº•ï¼šplayer_aaaa è„šæœ¬å˜é‡
            if not play_from:
                player_aaaa = self.extract_middle_text(html, 'player_aaaa={', '}', 0)
                if player_aaaa:
                    play_from.append("é»˜è®¤çº¿è·¯")
                    play_url.append("ç‚¹å‡»æ’­æ”¾$" + did)

            vod_item = {
                "vod_id": did,
                "vod_name": title,
                "vod_pic": pic,
                "vod_content": content,
                "vod_director": director,
                "vod_actor": actor,
                "vod_year": "",
                "vod_area": "",
                "vod_remarks": "",
                "vod_play_from": "$$$".join(play_from) if play_from else "é»˜è®¤",
                "vod_play_url": "$$$".join(play_url) if play_url else ""
            }
            videos.append(vod_item)

        except Exception as e:
            print(f"[çŸ­å‰§æ¨¡æ¿] detail error: {e}")
            videos.append({
                "vod_id": did,
                "vod_play_from": "åŠ è½½å¤±è´¥",
                "vod_play_url": "è¯·æ£€æŸ¥ç½‘ç»œæˆ–ç«™ç‚¹çŠ¶æ€"
            })

        result["list"] = videos
        return result

    # ==================== æ’­æ”¾é¡µ ====================
    def playerContent(self, flag, id, vipFlags):
        result = {}
        try:
            rsp = requests.get(id, headers=header_common, timeout=12, allow_redirects=True)
            rsp.raise_for_status()
            rsp.encoding = rsp.apparent_encoding or 'utf-8'
            html = rsp.text

            # å¤šæ–¹å¼æå–ç›´é“¾
            url_match = (
                re.search(r'"url"\s*:\s*"([^"]+\.m3u8[^"]*)"', html, re.I) or
                re.search(r"url\s*:\s*'([^']+\.m3u8[^']*)'", html, re.I) or
                re.search(r'src=["\']([^"\']+\.m3u8[^"\']*)["\']', html, re.I) or
                re.search(r'player_aaaa\s*=\s*({.+?})', html)
            )

            if url_match:
                if 'player_aaaa' in url_match.group(0):
                    # æ˜¯ player_aaaa å¯¹è±¡ï¼Œç›´æ¥è¿”å›åŸé¡µé¢è®©å®¢æˆ·ç«¯è§£æï¼ˆæœ€å®‰å…¨ï¼‰
                    final_url = id
                else:
                    final_url = url_match.group(1).replace('\\', '').replace('\\\\', '\\')
            else:
                final_url = fallback_jx + id  # å¤‡ç”¨è§£æå…œåº•

            result["parse"] = 0
            result["playUrl"] = ""
            result["url"] = final_url
            result["header"] = header_common

        except Exception as e:
            print(f"[çŸ­å‰§æ¨¡æ¿] player error: {e}")
            result["url"] = fallback_jx + id if fallback_jx else id

        return result

    # ==================== æœç´¢ ====================
    def searchContentPage(self, key, quick, page):
        result = {}
        videos = []
        pg = int(page) if page else 1

        possible_urls = [
            f"{base_url}/search/{quote(key)}/{pg}",
            f"{base_url}/vodsearch/{quote(key)}----------{pg}---.html",
            f"{base_url}/search.php?page={pg}&wd={quote(key)}",
            f"{base_url}/index.php?m=vod-search-wd-{quote(key)}-p-{pg}"
        ]

        search_url = ""
        for u in possible_urls:
            try:
                test_rsp = requests.get(u, headers=header_common, timeout=10)
                if test_rsp.status_code == 200 and len(test_rsp.text) > 2000:
                    search_url = u
                    break
            except:
                continue
        if not search_url:
            search_url = possible_urls[0]

        try:
            rsp = requests.get(search_url, headers=header_common, timeout=12)
            rsp.encoding = rsp.apparent_encoding or 'utf-8'
            soup = BeautifulSoup(rsp.text, "lxml")

            items = soup.select('.search-item, .video-item, .module-item, .v-item')
            for item in items:
                a = item.find('a')
                if not a: continue
                title = (a.get('title') or a.get_text(strip=True) or "").strip()
                if not title: continue

                videos.append({
                    "vod_id": urljoin(base_url, a['href']),
                    "vod_name": title,
                    "vod_pic": urljoin(base_url, item.find('img')['src'] if item.find('img') else ""),
                    "vod_remarks": ""
                })

        except Exception as e:
            print(f"[çŸ­å‰§æ¨¡æ¿] search error: {e}")

        result["list"] = videos
        result["page"] = pg
        result["pagecount"] = 9999
        result["limit"] = 90
        result["total"] = 999999
        return result

    def searchContent(self, key, quick, pg="1"):
        return self.searchContentPage(key, quick, pg)

    # ==================== æœ¬åœ°ä»£ç†ï¼ˆæ”¯æŒé˜²ç›—é“¾ï¼‰ ====================
    def localProxy(self, param):
        if param.get('type') in ["m3u8", "media", "ts"]:
            url = param.get('url', '')
            if url:
                try:
                    resp = requests.get(url, headers=header_common, timeout=12, stream=True)
                    if resp.status_code == 200:
                        return [200, resp.headers.get('Content-Type', 'video/MP2T'), resp.content]
                except:
                    pass
        return None


### ä½¿ç”¨è¯´æ˜ï¼ˆå†™æ–°æºåªéœ€ä¸‰æ­¥ï¼‰ï¼š
1. ä¿®æ”¹ `base_url` ä¸ºç›®æ ‡ç½‘ç«™åŸŸå
2. ä¿®æ”¹ `homeContent` ä¸­çš„ `classes` åˆ†ç±»ï¼ˆtype_id å¯¹åº” URL ä¸­çš„åˆ†ç±»å‚æ•°ï¼‰
3. å¦‚æœ‰ç‰¹æ®Šæƒ…å†µï¼Œå¾®è°ƒåˆ†ç±»/æœç´¢ URL æ ¼å¼ï¼ˆå·²å…¼å®¹ç»å¤§å¤šæ•°ï¼‰

### æœ¬æ¨¡æ¿å·²å®Œç¾è§£å†³ï¼š
- é˜²å°è·³è½¬ï¼ˆç™¾åº¦æ£€æµ‹ + è‡ªåŠ¨å¤‡ç”¨ï¼‰
- å¤šç§åˆ†é¡µ/æœç´¢æ ¼å¼å…¼å®¹
- å¤šçº¿è·¯ + player_aaaa å®Œç¾æ”¯æŒ
- æ‡’åŠ è½½å›¾ç‰‡ã€å¤‡æ³¨æ¸…ç†
- ç›´é“¾æå– + å¤‡ç”¨è§£æå…œåº•
- å…¨é“¾è·¯å¼‚å¸¸æ•è· + è¶…æ—¶æ§åˆ¶
- é˜²ç›—é“¾æœ¬åœ°ä»£ç†

ç»è¿‡å¤šè½®æ£€æŸ¥ï¼Œæ— è¯­æ³•é”™è¯¯ã€æ— ä½çº§bugã€å¯ç›´æ¥å¤åˆ¶æµ‹è¯•ã€‚  
è¿™æ˜¯ç›®å‰æœ€å…¨é¢ã€æœ€ç¨³å®šã€æœ€å®ç”¨çš„çŸ­å‰§å½±è§†æºæ¨¡æ¿ï¼Œç¥ä½ å†™æºä¸€æ¬¡æˆåŠŸï¼ğŸš€
